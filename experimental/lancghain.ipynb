{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embed and store\n",
    "from langchain.vectorstores.elasticsearch import ElasticsearchStore \n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings # We can also try Ollama embeddings\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elqm.utils.dataFinder import get_data_dir\n",
    "from elqm.backend.utils import get_es_connection\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/psaegert/Projects/elqm-INLPT-WS2023/elqm-raw/eur_lex_data\n",
      "/home/psaegert/Projects/elqm-INLPT-WS2023/elqm-raw/preprocessed\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = get_data_dir(\"eur_lex_data\")\n",
    "PREPROCESSED_DATA_DIR = get_data_dir(\"preprocessed\")\n",
    "\n",
    "print(os.path.abspath(DATA_DIR))\n",
    "print(os.path.abspath(PREPROCESSED_DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(os.listdir(DATA_DIR)):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(DATA_DIR, filename), 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        bs = BeautifulSoup(data['html'], 'html.parser')\n",
    "\n",
    "        # Get the text\n",
    "        text = bs.get_text()\n",
    "\n",
    "        data['text'] = text\n",
    "        del data['html']\n",
    "\n",
    "        with open(os.path.join(PREPROCESSED_DATA_DIR, filename), 'w') as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install jq\n",
    "schema = {\n",
    "    'jq_schema': '.text'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(PREPROCESSED_DATA_DIR, glob='**/*.json', show_progress=True, loader_cls=JSONLoader, loader_kwargs=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "print(f\"Split into {len(all_splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = get_es_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 docs, 1814 chunks, 1m 1.5s\n",
    "# 300 docs, 6127 chunks, 3m 29.1s\n",
    "# 508 docs, 12018 chunks, 6m 52s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REINDEX = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "# pip install gpt4all\n",
    "# Needs Ubuntu > 22.04 because of glibc\n",
    "\n",
    "if REINDEX:\n",
    "    # Clear the index\n",
    "    es.indices.delete(index=\"eurlex-langchain\", ignore=[400, 404])\n",
    "\n",
    "    start_time = time.time()\n",
    "    vectorstore = ElasticsearchStore.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings(), index_name=\"eurlex-langchain\", show_progress=True, es_connection=es)\n",
    "    print(f\"Embedding took {time.time() - start_time} seconds\")\n",
    "else:\n",
    "    vectorstore = ElasticsearchStore(index_name=\"eurlex-langchain\", es_connection=es, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "# question = \"What is the capital of france?\"\n",
    "# docs = vectorstore.similarity_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt\n",
    "# pip install langchainhub\n",
    "from langchain import hub\n",
    "QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer: [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(QA_CHAIN_PROMPT.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_CHAIN_PROMPT.messages[0].prompt.template = \"\"\"[INST]<<SYS>> You are ELQM, a helpful and specialized assistant for question-answering tasks in the domain of energy law.\n",
    "Use the following pieces of retrieved context comprised of EU regulations and other legal documents to answer the question.\n",
    "If you don't know the answer or the question cannot be answered with the context, admit that you cannot answer the question due to the limited available context.\n",
    "Furthermore, if the user asks a generic question or other situations occur, in which the context is not helpful, kindly remember the user of your purpose.\n",
    "In addition to the retrieved context, you may also consider the previous conversation history to interact with the user.\n",
    "Use three sentences maximum and keep the answer concise.<</SYS>> \n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer: [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import SimpleQueue, Empty\n",
    "q = SimpleQueue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import LLMResult\n",
    "from typing import Any, Union\n",
    "\n",
    "job_done = object() # signals the processing is done\n",
    "\n",
    "class StreamingGradioCallbackHandlerQ(BaseCallbackHandler):\n",
    "    def __init__(self, q: SimpleQueue):\n",
    "        self.q = q\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: dict[str, Any], prompts: dict[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when LLM starts running. Clean the queue.\"\"\"\n",
    "        while not self.q.empty():\n",
    "            try:\n",
    "                self.q.get(block=False)\n",
    "            except Empty:\n",
    "                continue\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        self.q.put(token)\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        self.q.put(job_done)\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "        self.q.put(job_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM model llama2\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "llm = Ollama(model=\"llama2\", verbose=True)#, callbacks=[FinalStreamingStdOutCallbackHandler(), StreamingGradioCallbackHandlerQ(q)])\n",
    "print(f\"Loaded LLM model {llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=5, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "class CustomConversationalRetrievalChain(ConversationalRetrievalChain):\n",
    "    def run(self, **kwargs):\n",
    "        # Execute the original logic\n",
    "        result = super().run(**kwargs)\n",
    "\n",
    "        # Access the question from the kwargs\n",
    "        # question = kwargs.get('question')\n",
    "\n",
    "        # Store the retrieved documents in the memory\n",
    "        # self.memory.save_context({\"question\": question}, {\"retrieved_docs\": result})  #FIXME: This is not working\n",
    "\n",
    "        # Yield the final answer\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = CustomConversationalRetrievalChain.from_llm(\n",
    "   llm,\n",
    "   retriever=vectorstore.as_retriever(),\n",
    "   combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "   memory=memory,\n",
    "   get_chat_history=lambda h : h,\n",
    "   # callbacks=[FinalStreamingStdOutCallbackHandler(), StreamingGradioCallbackHandlerQ(q)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_docs_chain_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    memory=memory,\n",
    "    get_chat_history=lambda h : h,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "# question = f\"How loud are air conditioners?\"\n",
    "# result = qa_chain({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psaegert/miniconda3/envs/elqm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, history):\n",
    "    result = qa_chain({\"question\": question, \"chat_history\": history})\n",
    "    return result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "\n",
    "# def answer_question_stream(question, history):\n",
    "#     # Empty the queue\n",
    "#     while not q.empty():\n",
    "#         try:\n",
    "#             q.get(block=False)\n",
    "#         except Empty:\n",
    "#             continue\n",
    "#     thread = Thread(target=qa_chain.run, kwargs={\"question\": question, \"chat_history\": history})\n",
    "#     thread.start()\n",
    "    \n",
    "#     result = \"\"\n",
    "#     while True:\n",
    "#         next_token = q.get(block=True) # Blocks until an input is available\n",
    "#         if next_token is job_done:\n",
    "#             break\n",
    "#         result += next_token\n",
    "#         yield result\n",
    "#     thread.join()\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "\n",
    "# def answer_question_stream_with_context(question, history):\n",
    "#     context = memory.load_memory_variables({})\n",
    "#     retrieved_docs = context['retrieved_docs']\n",
    "\n",
    "#     thread = Thread(target=qa_chain.run, kwargs={\"question\": question, \"chat_history\": history, \"retrieved_docs\": retrieved_docs})\n",
    "#     thread.start()\n",
    "    \n",
    "#     result = \"\"\n",
    "#     while True:\n",
    "#         next_token = q.get(block=True) # Blocks until an input is available\n",
    "#         if next_token is job_done:\n",
    "#             break\n",
    "#         result += next_token\n",
    "#         yield result\n",
    "#     thread.join()\n",
    "\n",
    "#     return qa_chain.memory.chat_memory.messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    "#chatbot {\n",
    "   height: 100%;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gr.Blocks(css=css) as demo:\n",
    "   gr.ChatInterface(fn=answer_question, title=\"ELQM\")\n",
    "\n",
    "demo.launch();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferWindowMemory(memory_key='chat_history')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elqm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
