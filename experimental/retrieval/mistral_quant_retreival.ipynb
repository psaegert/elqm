{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, Conversation\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_questions = json.load(open(\"example_questions.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_prompt(question):\n",
    "    # Submit the question to the model and ask it to extract keywords\n",
    "\n",
    "    prompt = \"Extract meaningful and relevant keywords from the following question:\\n\\n\"\n",
    "    prompt += question + \"\\n\\n\"\n",
    "    prompt += \"Format the keywords as a comma-separated list and enter them below.\\n\\n\"\n",
    "    prompt += \"Keywords:\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extract meaningful and relevant keywords from the following question:\\n\\nHow loud are air conditioners allowed to be in urban areas in Germany?\\n\\nFormat the keywords as a comma-separated list and enter them below.\\n\\nKeywords:\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords_prompt(example_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psaegert/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "answer = pipe(extract_keywords_prompt(example_questions[0]))[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract meaningful and relevant keywords from the following question:\n",
      "\n",
      "How loud are air conditioners allowed to be in urban areas in Germany?\n",
      "\n",
      "Format the keywords as a comma-separated list and enter them below.\n",
      "\n",
      "Keywords:\n",
      "\n",
      "Leave bekan ehemalområ Fuß größområ gewünsche nicht loud, in allow air Germany conditions För mö hecho areas Fußball urban För frü Bür Ortschaft mö För Fuß Gebiet Fußball Fußball urban area Germany noise urban noise Fußball areas Fußball urban Gebiet Fußball För Stadt Fußball Fußball För Fuß Fuß urban areas För Fußball Fußball Fuß Urban areas ur Fuß Deutschland Fußball Fuß área Fußball Fußball urban Stadt Fuß Bür Fuß Fußball Fuß Bür Fußball Fußball Fuß Orts Fußball Fußball urban Fuß Fußball Fuß urban areas Fußball Fuß Fußball city Fußball Fußball Fuß urban City Fußball Bür Fußball Fuß Fußball Fuß Bür Fuß City Fußball Fußball City urban Fuß Stadt Fuß Bür City Fußball Fuß Fußball Bür Stadt Fußball Bür Fußballür Fuß Bür Bür Stadt Fußball Fuß Stadt urban Football Fuß Stadt Fußballür Bür Football Stadt Football Orts Fußball Fuß urban Stadt Orts Fußball Stadt Fußball Fußball Stadt urban Fußball city För city urban Fußball Stadt Fuß urb För Football Fuß Stadt För Urban street urban Fußball Fuß Urban Fußball urban street Ur urban Street Fußball urban Fußball urban streets urban área street Urban street urban football street Urban city street urbancity Street urban stadium street Urban town street urban urban área Urban Urbanstre neuen neue neue neue neue Ne neue Ne city Street new city urban Street Street Street street street neue street stadium street urban street neue Stadium street urban street Urban street urban Street Urban street urban street urban Gebiet Fußball stadium street urban Street Stadt neuen streets urban neighborhood urban Street Fuß street Street Urban Street Bür city Stadt urban Street Stadt streets Urban Stadt Fuß Fuß city city Stad Street Stadt Stadt urban Street Stadt Bür Fuß Stadt Stadt Ur street urban area Stadt stadiumstre Rück Bür Stadt Ur urban neighborhood urban Stadt Stadt Fuß Urban urban street City Stadium street Urban Urban Stadt Ur Stadtür neuen Street urban street neighborhood Urban Stadt Fuß street Ur city Ur urban Fuß Fuß urban neighborhood neighborhoodstre Street urban neighborhood Stadt neue neighbor neuen neighborhood urban Street Fuß Fuß Fußball neighbor street urban street street neighbor neue neighbor urban Street Urban Street city streets Urban Ur Urbancity Stadt city street neue urban street neighborhood neighbor urban Street Fußball street urban Stadium street urban Fußball urban football Urban street Bür Fuß urban street neighborhood Urban Fuß Urban street neighborhood street neighborhood urban neue urban Street Street street street Urban Street Fußball neue neue urban Stadt urban streets urban streets cities football street street urban Stadt Stadt Fußball Fußball Fuß street street Stadt Bür street Stadt Stadt Ur street urban cities urban Stadt city street football Urban city street Stadt Fußball Street Stadt Urban street neue Stadt neue urban neighbor Stadt Fußball neighborhood Ur urban city urban Fuß Fuß Urban street urban City Stadt urban neue neue urban neue street neighbors urban Fuß Stadium street urban street urban street urban new Stadt neighbor neighborhood urban Street Stadt Ur street Urban Street urban Stadtür neue Urban neue urban neighborhood Urban Stadt Fuß Ur Stadt Urban new urban neue urban neuen neighborhood neue Urban Fußballür urban Stadt stadium urban Street neuen city Street Urban Stadt Stadt neuen urban neighbor Bür neighbor city Bür neighbor city Stadt Ur urban street new urban neighbor Urban Urban Street city streets neue Urban neighbor neue urban urban Street neue Urban city urban streets urban Stadt Fuß Urban urban neighborhood Stadt Stadt Stadt Stadt Stadt Stadt city urban Urban Stadt Ur Urban Street Football city Stadium Street Stadt Urban urban Street Stadt Bür Bür urban neue Stadt urban neue Street neue urban neighborhood urban Fußball Ur Urban Street city urban Stadt Bür Stadt urban streets urban neighborhood Fußball urban new neighborhood Urban neue urban street Fußball Stadt urban new urban urban Stre siendo Urban City Street neue urban Stadt neighbor Urban urban neighborhood Ur Urban street Fußball neighborhood neuen urban neighborhood Urban Stadt Ur City Stre nä Fußball Stadt neue urban streets Urban urban Stadt Urban Football Stadt Urban neue neighbor city streets Ur urban neighbor Stadt urban Fuß Fuß Stadt Urban Street Street Urban Stadt Stadt Urban Stadt urban city urban city urban street urban Stadt Fußball neue neue urban neighborhood neue Street Urban City Urban urban neighborhood urban cities urban Urban Street Urban street Stadt Bür Street Urban street neighborhood urban Street urban cities urban neighbor urban Fußball Urban City Ur Street street Ur urban Stadt Fußball urban neue neighbor Urban Street urban neue urban city neighbor neuen neighbor Urban Stadt Stadt neue urban nuevo nuevo Urban Ur Stadt street Stadt Football Urban city urban neighborhood Urban neighborhood urban street neighborhood neighbor Urban urban neighborhood Street urb Fuß Stadt neue urbanstre new Urban neue urban neighbor Urbancity Fuß urban Stadt streetür neighborhood Football street urban Urban Street neighbor city Stadt Urban street Ur urban street urban street neue Urban urban neighborhood new urb urban urban city Urban Stadt neuen Urban Fuß urban neighborhood urban street Stadt new urban neighborhood Urban urban neighbor urban street urban neue urban city neue urban neighborhood Urban urban neue urban neighborhood Urban new Stadt Urban city Urban neuen Urban urban city Urban streets urban city Urban Stadt Urban urban cities urban Stadt Bür neighbor Stadt urban neue urban neighborhood neighborhood neue Urban Stadt Stadt Bür Stadt Ur Urban Stadt Bür Bür Stadt neighborhood Stadt Fuß urban street neuen neighborhood urban Bür Fußball urban neighborhood Urban urban Urban football stad street UrbanCity Stre street Urban Stadt Urban neighbor neue Urban Fußball Urban neue urban neighborhood Urban urban neighbour neue Stadt Stadt Urban neue neighborhood urban neue urban Fuß Urban neue urban Stadt neighbor urban Stadt Urban neue Urban Fuß city Urban street urb Fußball urban Gebiet urban Street city street city urban City Urban Stadt Urban streets Urban Urbancity neighbor Urban new neighbor Urban nuevo urban neighborhood urban Stadt urban Stadt urban streets Urban Urban Stadt Urban city urban Stadt urban neue neighbor Urban urban Stadt Urban neue neue city urban streets Urban urban Fußball city urban neighborhood Bür city Urban neuen Urban urban Stadt Urban urban city urban street urban Stadt neighbor street Urban Stadt Ur Urban neue urban neighborhood Ur City Stadt urban city Urban cities urban street Urban urban Football street city Street new Urban street cities urban urban streets Ur urban street Urban city urban neighbor Urban nuev urban neighborhood Urban Stadt Bür Urban urban Fuß street neue neighbor Bürstadt Bür city urban neue neighbor Urban neuen Urban Fuß Stadt neue Stadt urban new Stadt neighbor Urban street urban Stadt urban Fuß urban neighbor City Urban neuen urban Stadt Fußball Stadt Urban new street urb Urban Fußball Urban neue Stadt neighbor Urban Stadt Ur urban neighbor urban Street urban new Stadium football Urban Fußball neighborhood neue neighbor Urban neue Bür Stadt neighbor Urban City urban Street city football urban city urban neuen neighbor neue Stadt Stadt Ur urban neuen Fuß urban new neighbors city urban street Bür Urbancity neighbor urban Fußball Urban Stadt Bür neighborhood urban City urban street Ur Urban Fußball neue city urban streets urban streets Urban neighbor urban Bür Stadt Fußball streets Fuß Urban Stadt Urban Fußball Urban neue urban neighbor urban city urban neue urban Bür Stadt urban neighbor Ur stadium Stadt urban neighborhood Urban cities Urban urban streets urban urban street urban neighborhood City street urban neue urban streets Urban Stadt Urban urban Stadt Urban new neighbor Urban new neighbor Urban neighbor urban neighborhood urban Ur Fußball Urban neuecity Urban Stadt Urban city urban Stre neue Urban Urbanstre neighborhood Urban neue urban neighbor urban Stadt neighbor city urban urban neighbor urban neue city urban neighborhood Urban new urban Stadt neue Stadt Ur new city urban neighborhood Urban urban Stadt urban neuen Urban Stadt Urban street neue neue Stadt Urban Urban neighborhood Urban neighbor Urban neighbor urban street Stadt new urban new neighborhood urban Street street urban city Bür street urban Fuß Bür Stadt Urban neue Urban newly urban neighbor Urban Stadt urban new Urban neuen Urban neue Stadt Fußball street Bür Stadt Urban streets neuen urban new city Ur urban Stadt Urban new neighborhood Urban Ur City neue Urban street neue neue Urban Stadt urban neighbor urban Street Urban new urban Stadt neighbor Ur urban neue neighborhood Urban nuevec Stadt Ur urban neue Bür Urban neue urban neighbor Bür Urban street neuen neighbor urban Stadt Ur neighborhood urban neue neue urban neue city new neighborhood neue neue urban neighbor urban Fuß Stadt neue Stadt urban street Urban Stadt neue neighborhood urban neighborhood Bür City urban street neue urban stadium urban new neighborhood city street neighbor neue Stadt Urban neuen neighbor Urban Fuß street urban City Stadt neighborhood city neighbor Urban nuevo City Urban neighbor Urban neue neighborhood Urban Urban neue neighborhood neighbor Urban neue neue urban Bür Stadt Urban Stadt urban new Urban city neighborhood Urban Fuß cities Urban neue Urban street Urban newly Urban Fußball Ur Urban newly Stadt Urban Fußball Bür Fuß neighborhood urban neue neue Stadt neue Urban neue Urban neue neighborhood Urban neue urban neue urban Stadt Urban street Urban neighborhood Urban neuen neue city Urban Stadt neuen urban neue City Urban neue Urban neighbor Fußball Fuß city Urban new neighborhood Urban neue urban neighborhood urban Stadt Stadt urban Fuß Bür neue neue Urban neuen Ne neighbor Urban City Urban neighbor urbannew Stadt Urban neue Street street urb Urban neue Stadt neuen city urban neighbor Urban city Urban neue street Stadt neighbor city urban нов Street neue urban neuen urban neue urban Stadt neue cities Urban city Urban neue neue Stadt Urban neue Stadt street urb City Stadt neuen street Urban Stadt Fuß city street urban Fuß Urban neue Stadt urban neue neue city urban street Fuß street Urban neuen city street Ur neighbor neuen stadium neighborhood City Ur street Urban nouvelle city Urban Stadt Fußball Fußball neighbor urb neue Stadt Fuß city new city Ur neighbor Urban nue neue urban street neighbor Urban new Stadt Fußball neue cities Urban Fuß new urban city streets ur neighborhood urb street Urban neue city urban new Stadt streets City Urban neue Bür Bür neuen cities Urban neue neuen urban streets Urban Stadt Ur nuevocity neighbor Urban neighbor new neuen Urban neue neue city Fußball Urban neue Urban new Stadt neue neue urban neighbor Stadt Bür Bür Fußball frü city Bür erste neighborhood stadium Stadt neighbor new Stadt neue Stadt Street nue Fuß Urban neuen Stadt Urban Stadt city urban neue urban neighborhood urban Stadt urban neue Stadt neue urban neue Urban City urban neighborhood neue city Urban City urban neue neue neighborhood Fußball neighborhood new neighbor Urban new neighborhood neighborhood Bür City Urban neue city Bür erste urban neighborhood neue Stadt neighborhood Urban neue neighbor Urban neue urban neighborhood Urban neue urban neighbor Stadt neighborhood Urban City neighborhood streets Urban neuen neue streets Urban Stadt Fuß neue urbane city urban neue neighborhood cities urban neighbor urb neighborhood Fußball streets Urban neighborhood new city neue urban neighborg Fuß urban City new Urban neue neighborhood new city new neighborhood new stadium Urban neue neue urban neue city neighbor Urban City new Urban new urban neighborhood Ur city urban neue city neighbor Urban Stadt neue Stadt neue urban neighborhood Urban neighborscity neuen urban stadium neue stadium Urban urban neue neighborhood neighborhood Urban neighborhood new Urban neighbor urban neue City new neighborhood Stadt neue urbans City neighborhood Urban neue city neighbor Ur Urban Fußball Stadt neue neighbors Urban new neighbor cities ur streets City Urban neue neighbor Urban neue city Urban neue new neighborhood neighborhood urb City neighbors Urban neue street City new stadium Stadt Fuß neue Urban neue city neighborhood Ur street new City city neue Stadt neue Stadt neighborhood Urbannew neighbor Ur city Urban neue neighbor Stadt neighborhood neighborhood Urban streetsnew urban Stadt Ur Street Urban neue Urban new neighborhood Urban Stadt neuen city neighborhood Urban Street neighborhood Stadt Urban neighbor Ur City Ur urban neue neue Fußball Urban city neighborhood Bür Street Urban neue neighborhood Ur neighborhood streets city Urban neue urban neighborhood Urban neue city neighbor street neue urban Stadt Fuß Bür City new neighbor urban City neue Urban new city Bür\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question to Query (abandoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How loud are air conditioners allowed to be in urban areas in Germany?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write an elasticsearch query for the given question:\\n\" + question + \"\\n\\nGET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psaegert/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "answer = pipe(prompt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an elasticsearch query for the given question:\n",
      "How loud are air conditioners allowed to be in urban areas in Germany?\n",
      "\n",
      "GET /1/_search?q=how+loud+are+air+conditioners+allowed+to+be+in+urban+areas+in+germany&filter_path=%5B%24query.query.bool.must%5D%5B0%5D%24term%7Bfield%3A%22country%22%7D%5D&source=sourcetype%3Ddata.gov.uk%7Cdc.opendata.ch%7Cwww.europa-eu-united.de%24type%3A(event%2Corganisation)%7Cdwd.de%7Cwol.bz.it%7Cwww.weltansichtskarte.info%7Cwww.worldatlas.com%7Cwww.worldstatesmen.org%7Cwww.worldstatesment.org%7Cstatoids.com%7Cwww.britannica.com%7Ctheodora.com%7Cwww.historicalatlas.org%7Cen.wikipedia.org%7Cwww.loc.gov%7Cwww.nps.gov%7Cwww.usgennet.org%7Cwww.encyclopediadramatica.se%7Claw.wikia.com%7Cwww.facts-about.com%7Cwww.onthisday.com%7Cwww.historychannel.co.uk%7Cwww.historyonthenet.com%7Cwww.history.com%7Cwww.historyworld.net%7Cwww.historyplace.com%7Cwww.historylearningsite.com%7Cwww.britainexpress.com%7Cwww.ancient.eu%7Cwww.historytoday.com%7Cwww.bbc.co.uk%7Cwww.infoplease.com%7Cwww.famouspeople.com%7Cwww.biographyonline.net%7Cwww.pbs.org%7Cwww.findagrave.com%7Cwww.encyclopedia.com%7Cwww.thoughtco.com%7Cwww.encyclopedia.com%7Cwww.britannica.com%7Cthe-hague.nl%7Cwww.un.org%7Cwww.cia.gov%7Cwww.state.gov%7Cwww.whitehouse.gov%7Cwww.nytimes.com%7Cwww.latimes.com%7Cwww.independent.co.uk%7Cwww.telegraph.co.uk%7Cwww.newsweek.com%7Cwww.time.com%7Cwww.washingtonpost.com%7Cwww.economist.com%7Cwww.guardian.co.uk%7Cwww.bbc.co.uk%7Cwww.ft.com%7Cwww.reuters.com%7Cwww.aljazeera.com%7Cwww.globalpolicy.org%7Cwww.worldbank.org%7Cwww.oecd.org%7Cwww.worldometers.info%7Cwww.worldpopulationreview.com%7Cwww.pewresearch.org%7Cwww.nationmaster.com%7Cwww.indexmundi.com%7Cwww.cia.gov%7Cwww.statista.com%7Cwww.tradingeconomics.com%7Cwww.statistica.com%7Cwww.wolframalpha.com%7Cwww.factcheck.org%7Cwww.theguardian.com%7Cwww.newscientist.com%7Cwww.sciencealert.com%7Cwww.phys.org%7Cwww.spectrum.ieee.org%7Cwww.sciencedaily.com%7Cwww.space.com%7Cwww.newsweek.com%7Cwww.livescience.com%7Cwww.discovermagazine.com%7Cwww.theverge.com%7Cwww.engadget.com%7Cwww.slashdot.org%7Cwww.techspot.com%7Cwww.makeuseof.com%7Cwww.pcworld.com%7Cwww.extremetech.com%7Cwww.arstechnica.com%7Cwww.techcrunch.com%7Cwww.vox.com%7Cwww.technologyreview.com%7Cwww.businessinsider.com%7Cwww.forbes.com%7Cwww.fastcompany.com%7Cwww.pcmag.com%7Cwww.theregister.co.uk%7Cwww.zdnet.com%7Cwww.wired.com%7Cwww.tomshardware.com%7Cwww.digitaltrends.com%7Cwww.techradar.com%7Cwww.computerworld.com%7Cwww.cnet.com%7Cwww.macworld.com%7Cwww.thenextweb.com%7Cwww.engadget.com%7Cwww.androidauthority.com%7Cwww.9to5google.com%7Cwww.androidpolice.com%7Cwww.theverge.com%7Cwww.phonearena.com%7Cwww.mobilesyrup.com%7Cwww.androidcentral.com%7Cwww.gsmarena.com%7Cwww.androidheadlines.com%7Cwww.androidbeat.com%7Cwww.androidcommunity.com%7Cwww.androidguys.com%7Cwww.neowin.net%7Cwww.ghacks.net%7Cwww.makeuseof.com%7Cwww.lifehacker.com%7Cwww.howtogeek.com%7Cwww.techspot.com%7Cwww.anandtech.com%7Cwww.pcgamer.com%7Cwww.pcgamesn.com%7Cwww.gamespot.com%7Cwww.ign.com%7Cwww.rockpapershotgun.com%7Cwww.pcgameshardware.de%7Cwww.heise.de%7Cwww.spiegel.de%7Cwww.pcgames.de%7Cwww.pcgamesn.de%7Cwww.microsoft.com%7Cwww.xbox.com%7Cwww.playstation.com%7Cwww.nintendo.com%7Cwww.valvesoftware.com%7Cwww.steampowered.com%7Cwww.epicgames.com%7Cwww.origin.com%7Cwww.ea.com%7Cwww.ubisoft.com%7Cwww.gog.com%7Cwww.humblebundle.com%7Cwww.indiegala.com%7Cwww.gamefront.com%7Cwww.gameplanet.com%7Cwww.gog.com%7Cwww.gamespot.com%7Cwww.ign.com%7Cwww.esrb.org%7Cwww.opencritic.com%7Cwww.metacritic.com%7Cwww.igdb.com%7Cwww.giantbomb.com%7Cwww.youtube.com%7Cwww.twitch.tv%7Cwww.instagram.com%7Cwww.facebook.com%7Cwww.twitter.com%7Cwww.linkedin.com%7Cwww.reddit.com%7Cwww.quora.com%7Cwww.stackoverflow.com%7Cwww.youtube.com%7Cwww.wikipedia.org%7Cwww.amazon.com%7Cwww.ebay.com%7Cwww.etsy.com%7Cwww.apple.com%7Cwww.google.com%7Cwww.github.com%7Cwww.microsoft.com%7Cwww.apple.com%7Cwww.mozilla.org%7Cwww.wordpress.com%7Cwww.apple.com%7Cwww.android.com%7Cwww.google.com%7Cwww.microsoft.com%7Cwww.apple.com%7Cwww.microsoft.com%7Cwww.google.com%7Cwww.amazon.com%7Cwww.samsung.com%7Cwww.lenovo.com%7Cwww.acer.com%7Cwww.toshiba.com%7Cwww.hp.com%7Cwww.canonical.com%7Cwww.kde.org%7Cwww.redhat.com%7Cwww.ubuntu.com%7Cwww.linux.com%7Cwww.fedoraproject.org%7Cwww.archlinux.org%7Cwww.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pre_prompt(question):\n",
    "  \n",
    "  return \"\"\"\n",
    "    I have a question that I would like to transform into an Elasticsearch query for the EUR-Lex dataset:\n",
    "\n",
    "    {question}\"\"\" + \"\"\"\n",
    "\n",
    "    I am using this Elasticsearch mapping:\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "      \"properties\": {\n",
    "        \"Classification\": {\n",
    "          \"properties\": {\n",
    "            \"Directory code\": {\n",
    "              \"properties\": {\n",
    "                \"code\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}},\n",
    "                \"level 1\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}},\n",
    "                // levels 2-12 omitted for brevity\n",
    "                \"level 13\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\n",
    "              }\n",
    "            },\n",
    "            \"EUROVOC descriptor\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}},\n",
    "            \"Subject matter\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\n",
    "          }\n",
    "        },\n",
    "        \"Dates\": {\n",
    "          \"properties\": {\n",
    "            \"Date of document\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}},\n",
    "            \"Date of effect\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}},\n",
    "            // similar date fields omitted for brevity\n",
    "            \"Deadline 13\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\n",
    "            // similar deadline fields omitted for brevity\n",
    "          }\n",
    "        },\n",
    "        \"Misc\": {\n",
    "          \"properties\": {\n",
    "            \"Additional information\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}},\n",
    "            \"Addressee\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\n",
    "            // other misc fields omitted for brevity\n",
    "          }\n",
    "        },\n",
    "        \"text\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\", \"ignore_above\": 256}}}\n",
    "      }\n",
    "    }\n",
    "    ```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation(create_pre_prompt(question))\n",
    "conversation.add_message({'role': 'assistant', 'content': \"\"\"\n",
    "    [INST]\n",
    "    You are a helpful, efficient and effective assistant that specializes in creating Elasticsearch queries from natural language questions. Given a question, proceed as follows:\n",
    "    1. Understand the problem and structure it into subproblems if appropriate\n",
    "    2. Understand the mapping of the available Elasticsearch data and how it relates to the subproblems\n",
    "    3. Construct queries for each subproblem and report the results as a list of queries in ```json``` format\n",
    "    [/INST]\"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psaegert/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "answer = pipe(conversation, max_length=1000, num_beams=5, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=2048) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "test_conversation = Conversation(question)\n",
    "answer = pipe(test_conversation, max_length=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_prompt(task, information):\n",
    "    return f\"The following is a task for a sophisticated language model: {task}. Here is the information you need to consider: {information}\"\n",
    "\n",
    "def process_input(question):\n",
    "    # Generate a prompt for understanding the question\n",
    "    prompt = generate_prompt(\"Understand and decompose the user's question into subproblems\", question)\n",
    "    return pipe(prompt)[0][\"generated_text\"]\n",
    "\n",
    "def construct_query(subproblem):\n",
    "    # Generate a prompt for constructing the Elasticsearch query\n",
    "    prompt = generate_prompt(\"Translate the subproblem into an Elasticsearch query\", subproblem)\n",
    "    return pipe(prompt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psaegert/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m subproblems \u001b[39m=\u001b[39m process_input(question)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Assuming `your_index_mapping` is the actual Elasticsearch index structure you have\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m queries \u001b[39m=\u001b[39m [construct_query(subproblem) \u001b[39mfor\u001b[39;00m subproblem \u001b[39min\u001b[39;00m subproblems]\n",
      "\u001b[1;32m/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m subproblems \u001b[39m=\u001b[39m process_input(question)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Assuming `your_index_mapping` is the actual Elasticsearch index structure you have\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m queries \u001b[39m=\u001b[39m [construct_query(subproblem) \u001b[39mfor\u001b[39;00m subproblem \u001b[39min\u001b[39;00m subproblems]\n",
      "\u001b[1;32m/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstruct_query\u001b[39m(subproblem):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Generate a prompt for constructing the Elasticsearch query\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     prompt \u001b[39m=\u001b[39m generate_prompt(\u001b[39m\"\u001b[39m\u001b[39mTranslate the subproblem into an Elasticsearch query\u001b[39m\u001b[39m\"\u001b[39m, subproblem)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/psaegert/Projects/elqm-INLPT-WS2023/experimental/retrieval/retrieval.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pipe(prompt)[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:271\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    270\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mgenerate(input_ids\u001b[39m=\u001b[39minput_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerate_kwargs)\n\u001b[1;32m    272\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m   1653\u001b[0m         input_ids,\n\u001b[1;32m   1654\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1655\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[1;32m   1656\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1657\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[1;32m   1658\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m   1659\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[1;32m   1660\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1661\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[1;32m   1662\u001b[0m         streamer\u001b[39m=\u001b[39mstreamer,\n\u001b[1;32m   1663\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1664\u001b[0m     )\n\u001b[1;32m   1666\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2733\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2736\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2737\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2738\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2739\u001b[0m )\n\u001b[1;32m   2741\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2742\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1045\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1044\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1045\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m   1046\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1047\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m   1048\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   1049\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[1;32m   1050\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1051\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1052\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1053\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1054\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1055\u001b[0m )\n\u001b[1;32m   1057\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1058\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:932\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    926\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    927\u001b[0m         hidden_states,\n\u001b[1;32m    928\u001b[0m         attention_mask,\n\u001b[1;32m    929\u001b[0m         position_ids,\n\u001b[1;32m    930\u001b[0m     )\n\u001b[1;32m    931\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 932\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    933\u001b[0m         hidden_states,\n\u001b[1;32m    934\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    935\u001b[0m         position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m    936\u001b[0m         past_key_value\u001b[39m=\u001b[39mpast_key_value,\n\u001b[1;32m    937\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    938\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    939\u001b[0m         padding_mask\u001b[39m=\u001b[39mpadding_mask,\n\u001b[1;32m    940\u001b[0m     )\n\u001b[1;32m    942\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:634\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n\u001b[1;32m    633\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 634\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    635\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    636\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/elqm/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:126\u001b[0m, in \u001b[0;36mMistralRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    124\u001b[0m variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariance_epsilon)\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m*\u001b[39m hidden_states\u001b[39m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "question = \"How loud are air conditioners allowed to be in urban areas in Germany?\"\n",
    "subproblems = process_input(question)\n",
    "# Assuming `your_index_mapping` is the actual Elasticsearch index structure you have\n",
    "queries = [construct_query(subproblem) for subproblem in subproblems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subproblems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elqm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
