{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR:  /home/computerman/Desktop/NLPT/elqm-INLPT-WS2023/elqm-raw/eur_lex_data\n",
      "PREPROCESSED_DATA_DIR:  /home/computerman/Desktop/NLPT/elqm-INLPT-WS2023/elqm-raw/preprocessed\n"
     ]
    }
   ],
   "source": [
    "from elqm.utils.dataFinder import get_data_dir\n",
    "from elqm.backend.utils import get_es_connection\n",
    "import os\n",
    "import time\n",
    "\n",
    "DATA_DIR = os.path.abspath(get_data_dir(\"eur_lex_data\"))\n",
    "PREPROCESSED_DATA_DIR = os.path.abspath(get_data_dir(\"preprocessed\"))\n",
    "\n",
    "print(\"DATA_DIR: \", DATA_DIR)\n",
    "print(\"PREPROCESSED_DATA_DIR: \", PREPROCESSED_DATA_DIR)\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Initilize the LLM model\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "questionPrompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"Answer the question based only on the following context and on the conversation history:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "systemString = \"\"\"You are ELQM, a helpful and specialized assistant for question-answering tasks \\\n",
    "in the domain of energy law. Use the following pieces of retrieved context comprised of EU \\\n",
    "regulations and other legal documents to answer the question. If you don't know the answer \\\n",
    "or the question cannot be answered with the context, admit that you cannot answer the \\\n",
    "question due to the limited available context. Furthermore, if the user asks a generic \\\n",
    "question or other situations occur, in which the context is not helpful, kindly remember the \\\n",
    "user of your purpose. Your answers should not include any racist, sexist and toxic content.\"\"\"\n",
    "\n",
    "systemMessage = SystemMessage(content=systemString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "historyPrompt = MessagesPlaceholder(variable_name=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = (systemMessage + historyPrompt + questionPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt.format_messages(context=\"Hello\", question=\"Test\", history=\"This is the history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "embeddings = GPT4AllEmbeddings();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n",
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\",\n",
    "     \"Josh worked at John Deere\",\n",
    "     \"Sabrina worked at Google\",\n",
    "     \"Jasmin worked at Continental\",\n",
    "     \"James worked at Microsoft\",\n",
    "     \"Joshua worked at the local mechanics shop\",\n",
    "     \"Nikita worked at Rechner Sensors\",\n",
    "     \"Nikita worked at the University\",\n",
    "     \"Kira worked at the school\",\n",
    "     \"Uli worked at the airport\",\n",
    "     \"Maria worked at Mitsubishi Chemical\"],\n",
    "    embedding=GPT4AllEmbeddings()\n",
    ")\n",
    "\n",
    "# As default the retreiver outputs 4 documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Where did Harrison work?\"\n",
    "docs = retriever.invoke(query)\n",
    "print(type(docs))\n",
    "print(\"Number of docs:\", len(docs))\n",
    "print()\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Document {i}\")\n",
    "    print(\"Content\", doc.page_content)\n",
    "    print(\"Metadata:\", doc.metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug output is good good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following two seem so be equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain2 = setup_and_retrieval | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to make a class with different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "result = setup_and_retrieval.invoke(\"Where did Harrison work?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "setup_and_retrieval_runnable = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "@chain\n",
    "def custom_chain(question):\n",
    "    retrival_output = setup_and_retrieval.invoke(question)\n",
    "    prompt_output = prompt.invoke(retrival_output)\n",
    "    llm_output = llm.invoke(prompt_output)\n",
    "    anwser = StrOutputParser().invoke(llm_output)\n",
    "    return anwser, retrival_output, prompt_output, llm_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chain.invoke(\"Where did Harrison work and what did he do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class is nice but it is crap that the chain here does not show because it is defined explicetly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a parallel pass through that passes retrieved data all the way through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_core.prompt_values\n",
    "\n",
    "def clean_retriever_string(retrievedDocuments: langchain_core.prompt_values.ChatPromptValue):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain31 = retriever\n",
    "\n",
    "chain32 = {\"context\": retriever,\n",
    "           \"question\": RunnablePassthrough()} | prompt\n",
    "\n",
    "chain33 = (llm | StrOutputParser())\n",
    "\n",
    "chain34 = (\n",
    "    chain32 | RunnableParallel(completly_processed=chain33,\n",
    "                               prompted=RunnablePassthrough())\n",
    ")\n",
    "\n",
    "chain35 = RunnableParallel(completly_and_prompted=chain34,\n",
    "                           retreived=chain31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain32.invoke(\"Where did Harrison work and what did he do?\")\n",
    "print(type(result))\n",
    "final_output = result.to_messages()\n",
    "print(type(final_output))\n",
    "print(len(final_output))\n",
    "print(final_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain35.invoke(\"Where did Harrison work and what did he do?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        +-------------------------------------------------+                   \n",
      "                        | Parallel<completly_and_prompted,retreived>Input |                   \n",
      "                        +-------------------------------------------------+                   \n",
      "                                     ****                      *****                          \n",
      "                                 ****                               *****                     \n",
      "                               **                                        *****                \n",
      "         +---------------------------------+                                  ***             \n",
      "         | Parallel<context,question>Input |                                    *             \n",
      "         +---------------------------------+                                    *             \n",
      "                  ***            ***                                            *             \n",
      "                **                  **                                          *             \n",
      "              **                      **                                        *             \n",
      "+----------------------+          +-------------+                               *             \n",
      "| VectorStoreRetriever |          | Passthrough |                               *             \n",
      "+----------------------+          +-------------+                               *             \n",
      "                  ***            ***                                            *             \n",
      "                     **        **                                               *             \n",
      "                       **    **                                                 *             \n",
      "         +----------------------------------+                                   *             \n",
      "         | Parallel<context,question>Output |                                   *             \n",
      "         +----------------------------------+                                   *             \n",
      "                           *                                                    *             \n",
      "                           *                                                    *             \n",
      "                           *                                                    *             \n",
      "                +--------------------+                                          *             \n",
      "                | ChatPromptTemplate |                                          *             \n",
      "                +--------------------+                                          *             \n",
      "                           *                                                    *             \n",
      "                           *                                                    *             \n",
      "                           *                                                    *             \n",
      "   +---------------------------------------------+                              *             \n",
      "   | Parallel<completly_processed,prompted>Input |                              *             \n",
      "   +---------------------------------------------+                              *             \n",
      "                   ***           **                                             *             \n",
      "                  *                **                                           *             \n",
      "                **                   **                                         *             \n",
      "        +--------+                     **                                       *             \n",
      "        | Ollama |                      *                                       *             \n",
      "        +--------+                      *                                       *             \n",
      "             *                          *                                       *             \n",
      "             *                          *                                       *             \n",
      "             *                          *                                       *             \n",
      "    +-----------------+          +-------------+                                *             \n",
      "    | StrOutputParser |          | Passthrough |                                *             \n",
      "    +-----------------+          +-------------+                                *             \n",
      "                   ***           **                                             *             \n",
      "                      *        **                                               *             \n",
      "                       **    **                                                 *             \n",
      "   +----------------------------------------------+                 +----------------------+  \n",
      "   | Parallel<completly_processed,prompted>Output |                 | VectorStoreRetriever |  \n",
      "   +----------------------------------------------+                 +----------------------+  \n",
      "                                     ****                      *****                          \n",
      "                                         ****             *****                               \n",
      "                                             **        ***                                    \n",
      "                        +--------------------------------------------------+                  \n",
      "                        | Parallel<completly_and_prompted,retreived>Output |                  \n",
      "                        +--------------------------------------------------+                  \n"
     ]
    }
   ],
   "source": [
    "chain35.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "really really nice. this is exactly what we want. Now add memory\n",
    "https://www.reddit.com/r/LangChain/comments/18yovcm/please_help_with_langchain_want_both_document/.\n",
    "TODO for later: How to convert the retriever output into something nice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding conversation memory to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.utils import ConfigurableFieldSpec\n",
    "from typing import Optional\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> ChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = ChatMessageHistory()\n",
    "    return store[(user_id, conversation_id)]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain35,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"user_id\": \"123\", \"conversation_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain35,\n",
    "    RedisChatMessageHistory,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import redis python package. Please install it with `pip install redis`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/NLPT/elqm-INLPT-WS2023/elqmVenv/lib/python3.11/site-packages/langchain_community/chat_message_histories/redis.py:28\u001b[0m, in \u001b[0;36mRedisChatMessageHistory.__init__\u001b[0;34m(self, session_id, url, key_prefix, ttl)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/NLPT/elqm-INLPT-WS2023/elqmVenv/lib/python3.11/site-packages/redis/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asyncio  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackoff\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m default_backoff\n",
      "File \u001b[0;32m~/Desktop/NLPT/elqm-INLPT-WS2023/elqmVenv/lib/python3.11/site-packages/redis/asyncio/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Redis, StrictRedis\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RedisCluster\n",
      "File \u001b[0;32m~/Desktop/NLPT/elqm-INLPT-WS2023/elqmVenv/lib/python3.11/site-packages/redis/asyncio/client.py:32\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     _RedisCallbacks,\n\u001b[1;32m     28\u001b[0m     _RedisCallbacksRESP2,\n\u001b[1;32m     29\u001b[0m     _RedisCallbacksRESP3,\n\u001b[1;32m     30\u001b[0m     bool_ok,\n\u001b[1;32m     31\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     Connection,\n\u001b[1;32m     34\u001b[0m     ConnectionPool,\n\u001b[1;32m     35\u001b[0m     SSLConnection,\n\u001b[1;32m     36\u001b[0m     UnixDomainSocketConnection,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lock\n",
      "File \u001b[0;32m~/Desktop/NLPT/elqm-INLPT-WS2023/elqmVenv/lib/python3.11/site-packages/redis/asyncio/connection.py:32\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01masync_timeout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timeout \u001b[38;5;28;01mas\u001b[39;00m async_timeout\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Retry\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'async_timeout'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain_with_history\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhere did Harrison work and what did he do?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NLPT/elqm-INLPT-WS2023/elqmVenv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3598\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   3591\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3592\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   3593\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3594\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   3595\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m   3596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   3597\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m-> 3598\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3599\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   3600\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/NLPT/elqm-INLPT-WS2023/elqmVenv/lib/python3.11/site-packages/langchain_core/runnables/history.py:382\u001b[0m, in \u001b[0;36mRunnableWithMessageHistory._merge_configs\u001b[0;34m(self, *configs)\u001b[0m\n\u001b[1;32m    378\u001b[0m parameter_names \u001b[38;5;241m=\u001b[39m _get_parameter_names(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_history)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expected_keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;66;03m# If arity = 1, then invoke function by positional arguments\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m     message_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_session_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# otherwise verify that names of keys patch and invoke by named arguments\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(expected_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(parameter_names):\n",
      "File \u001b[0;32m~/Desktop/NLPT/elqm-INLPT-WS2023/elqmVenv/lib/python3.11/site-packages/langchain_community/chat_message_histories/redis.py:30\u001b[0m, in \u001b[0;36mRedisChatMessageHistory.__init__\u001b[0;34m(self, session_id, url, key_prefix, ttl)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mredis\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import redis python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install redis`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mredis_client \u001b[38;5;241m=\u001b[39m get_client(redis_url\u001b[38;5;241m=\u001b[39murl)\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import redis python package. Please install it with `pip install redis`."
     ]
    }
   ],
   "source": [
    "chain_with_history.invoke(\"Where did Harrison work and what did he do?\",\n",
    "                          config={\"configurable\": {\"session_id\": \"foo\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turns out this is legacy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5, memory_key=\"chat_history\")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    memory=memory,\n",
    "    get_chat_history=lambda h : h,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.get_prompts(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Where did Harrison work and what did he do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"where did harrison work?\"\n",
    "history = \"\"\n",
    "result = qa_chain.invoke({\"question\": question, \"chat_history\": history})\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elqmVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
