{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dynaconf import Dynaconf\n",
    "\n",
    "from elqm import ELQMPipeline\n",
    "from elqm.utils import get_configs_dir, get_data_dir, get_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache key: 256_bge_fn_meta_enrich\n",
      "No cache found for 256_bge_fn_meta_enrich\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading documents: 100%|██████████| 550/550 [00:00<00:00, 1557.61it/s]\n",
      "Transform Footnotes in HTML Docs: 100%|██████████| 550/550 [00:15<00:00, 35.12it/s]\n",
      "Removing HTML tags: 100%|██████████| 550/550 [00:10<00:00, 51.27it/s]\n",
      "Enriching Text with Metadata: 100%|██████████| 68126/68126 [00:00<00:00, 794206.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_0.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_1.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_2.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_3.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_4.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_5.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_6.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_7.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_8.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_9.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_10.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_11.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_12.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_13.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32003D0462_14.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32001D0762_0.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32001D0762_1.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32001D0762_2.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32001D0762_3.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32001D0762_4.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32001D0762_5.json'. Will be skipped.\n",
      "Warning: Metadata key 'Classification.EUROVOC descriptor' not found in document '32001D0762_6.json'. Will be skipped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving documents: 100%|██████████| 68126/68126 [00:02<00:00, 30568.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68126/68126 [02:12<00:00, 515.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Document Loader DirectoryLoader\n",
      "Loaded 68126 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psaegert/miniconda3/envs/elqm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Embedding HuggingFaceEmbeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating FAISS vectorstores: 100%|██████████| 682/682 [02:55<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Retriever VectorStoreRetriever\n"
     ]
    }
   ],
   "source": [
    "elqm = ELQMPipeline(config=Dynaconf(settings_files=os.path.join(get_configs_dir(), \"256_nlc_bge_meta_fn_enrich.yaml\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68126/68126 [02:06<00:00, 538.82it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = elqm.loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elqm.eval.oracle import generate_question_answer_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community market. The Member States shall notify those provisions to the Commission by 20 November 2010 and shall notify it without delay of any subsequent amendment affecting them.\n",
      "Article 21\n",
      "Review\n",
      "Not later than 2012, the Commission shall review the\n"
     ]
    }
   ],
   "source": [
    "print(documents[2].metadata['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{QUESTION 1}: What is the deadline for notifying the Commission of any amendments to the provisions related to the Community market? {ANSWER 1}: Not later than 2012, the Commission shall review the provisions."
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('What is the deadline for notifying the Commission of any amendments to the provisions related to the Community market? ',\n",
       "  'Not later than 2012, the Commission shall review the provisions.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_question_answer_pairs(\n",
    "    context=documents[2].metadata['text'],\n",
    "    prompt=None,  # default\n",
    "    question_type=None,  # default\n",
    "    n=1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tempfile\n",
    "import textwrap\n",
    "\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from elqm.utils import get_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUSTION_TYPES = {\n",
    "    \"confirmation\": \"Focus only on confirmation questions, i.e. questions that can be answered with a yes or no.\",\n",
    "    \"factoid\": \"Focus only on factoid questions, that usually begin with a who, what, where, when, why, or how.\",\n",
    "    \"list\": \"Focus only on list questions, i.e. questions that are answered with a list of items.\",\n",
    "    \"causal\": \"Focus only on causal questions, i.e. questions that begin with why or how.\",\n",
    "    \"hypothetical\": \"Focus only on hypothetical questions, i.e. questions that ask what if.\",\n",
    "    \"complex\": \"Focus only on complex questions, i.e. questions that require multi-step reasoning and comparisons.\",\n",
    "    \"default\": \"\"\n",
    "}\n",
    "\n",
    "\n",
    "def generate_question_answer_pairs(context: str, prompt: str | None = None, question_type: str | None = None, n: int = 1, verbose: bool = False) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Generate question-answer pairs from a given context using the Ollama model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    context : str\n",
    "        The context from which to generate the question-answer pairs.\n",
    "    prompt : str, optional\n",
    "        The prompt to use for the Ollama model. If None, a default prompt is used.\n",
    "    question_type : str, optional\n",
    "        The type of questions to generate. If None, any type of question is generated.\n",
    "    n : int, optional\n",
    "        The number of question-answer pairs to generate.\n",
    "    verbose : bool, optional\n",
    "        Whether to print the output of the Ollama model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, str]]\n",
    "        A list of question-answer pairs.\n",
    "    \"\"\"\n",
    "    if prompt is None:\n",
    "\n",
    "        question_type_prompt = QUSTION_TYPES.get(question_type, QUSTION_TYPES[\"default\"])  # type: ignore\n",
    "\n",
    "        prompt = textwrap.dedent(f'''You are an oracle system or a Retrieval Augmented Generation System, that guesses questions that would answered by a particular exerpt of text.\n",
    "            Fiven the following exerpt of text, generate {n} question{\"s\" if n > 1 else \"\"} that can be answered by the exerpt of text.\n",
    "            ```\n",
    "            {context}\n",
    "            ```\n",
    "             {question_type_prompt} \\\n",
    "            Format the pairs as follows:\n",
    "            ```\n",
    "            {{QUESTION i}}: <question i> {{ANSWER i}}: <answer i>\n",
    "            ```\n",
    "            Do not add any additional newlines between the pairs. Directly continue with the answer after the question.\n",
    "            Only add a newline between the pairs (after the answer) if you want to add more pairs.\n",
    "            Do not deviate from this format, since it will be used to extract the questions with the following regex: `{{QUESTION \\\\d+}}: .+ {{ANSWER \\\\d+}}: .+`\n",
    "            ''')\n",
    "\n",
    "    # Clear the message history by initializing a new Ollama instance\n",
    "    ollama = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"llama2\",\n",
    "        verbose=True,\n",
    "        stop=[\"<|im_end|>\"]\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        # Stream the output\n",
    "        response = \"\"\n",
    "        for token in ollama.stream(prompt):\n",
    "            response += token\n",
    "            print(token, end=\"\")\n",
    "    else:\n",
    "        # Generate the question-answer pairs\n",
    "        response = ollama.invoke(prompt)\n",
    "\n",
    "    # Filter out the question-answer pairs\n",
    "    qa_strings = re.findall(r'{QUESTION \\d+}: .+ {ANSWER \\d+}: .+', response)\n",
    "\n",
    "    # Extract the question and answer from the question-answer pairs\n",
    "    qa_pairs = []\n",
    "    for qa_pair in qa_strings:\n",
    "        question, answer = qa_pair.split(\"{ANSWER\")[0].split(\"}: \")[1], qa_pair.split(\"{ANSWER\")[1].split(\"}: \")[1]\n",
    "\n",
    "        # Skip empty questions or answers\n",
    "        if question != \"\" and answer != \"\":\n",
    "            qa_pairs.append((question, answer))\n",
    "        elif verbose:\n",
    "            print(f\"Question or answer is empty: {qa_pair}\")\n",
    "\n",
    "    # Check if any question-answer pairs were generated\n",
    "    if len(qa_pairs) == 0 and verbose:\n",
    "        print(f\"No question-answer pairs were generated: {qa_pairs}\")\n",
    "\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_oracle_dataset(documents: list[Document], question_type: str | list[str] | None = None, n_questions_per_type: int = 1, verbose: bool = False) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Generate a dataset of question-answer pairs from a given directory of data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    documents : list[Document]\n",
    "        The list of documents to generate question-answer pairs from.\n",
    "    question_type : str | list[str], optional\n",
    "        The type of question to generate. If None, all types are used, by default None\n",
    "    n_questions_per_type : int, optional\n",
    "        The number of questions to generate per question type, by default 1\n",
    "    verbose : bool, optional\n",
    "        Whether to print progress, by default False\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(question_type, str):\n",
    "        question_type = [question_type]\n",
    "    elif question_type is None:\n",
    "        question_type = list(QUSTION_TYPES.keys())\n",
    "\n",
    "    # Generate question-answer pairs for each document\n",
    "    for document in tqdm(documents, desc=\"Generating question-answer pairs\", disable=not verbose):\n",
    "\n",
    "        # If a raw text is available (as in the case of enrichment), use the raw text stored in the metadata\n",
    "        if 'text' in document.metadata:\n",
    "            context = document.metadata['text']\n",
    "        # Otherwise, use the page content\n",
    "        else:\n",
    "            context = document.page_content\n",
    "\n",
    "        document.metadata['oracle_pairs'] = []\n",
    "\n",
    "        for qt in question_type:\n",
    "            pairs = generate_question_answer_pairs(context, question_type=qt, n=n_questions_per_type, verbose=False)\n",
    "            for pair in pairs:\n",
    "                document.metadata['oracle_pairs'].append({'question': pair[0], 'answer': pair[1], 'type': qt})\n",
    "                \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating question-answer pairs: 100%|██████████| 10/10 [00:43<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "oracle_documents = generate_oracle_dataset(documents[:10], n_questions_per_type=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Document content:\\nPortuguese Republic.\\nDone at Lisbon on the seventeenth day of December in the year one thousand nine hundred and ninety-four.\\nFait à Lisbonne, le dix-sept décembre mil neuf cent quatre-vingt-quatorze.\\nGeschehen zu Lissabon am siebzehnten Dezember\\nEUROVOC descriptor:environmental policy,energy policy,ECSC,EAEC,accession to an agreement,European charter\\nDate of document:23/09/1997', metadata={'source': '/home/psaegert/Projects/elqm-INLPT-WS2023/cache/256_bge_fn_meta_enrich/preprocessed_documents/31998D0181_612.json', 'seq_num': 1, 'date_of_document': '1997-09-23', 'date_of_effect': '1997-09-23', 'date_of_signature': '', 'date_of_end_of_validity': 'No end date', 'author': 'Council of the European Union, European Commission', 'form': 'Decision', 'internal_comment': '', 'depositary': '', 'CELEX_ID': '31998D0181', 'text': 'Portuguese Republic.\\nDone at Lisbon on the seventeenth day of December in the year one thousand nine hundred and ninety-four.\\nFait à Lisbonne, le dix-sept décembre mil neuf cent quatre-vingt-quatorze.\\nGeschehen zu Lissabon am siebzehnten Dezember', 'oracle_pairs': [{'question': 'Is the document signed in Portuguese? ', 'answer': 'Yes.', 'type': 'confirmation'}, {'question': 'Who signed the document? ', 'answer': 'The document was signed by the President of the Portuguese Republic.', 'type': 'factoid'}, {'question': 'What country is the document signed in? ', 'answer': 'Portuguese Republic.', 'type': 'list'}, {'question': 'Why was the document signed in Lisbon on December 17, 1994? ', 'answer': 'Because it was done à Lisbonne, le dix-sept décembre mil neuf cent quatre-vingt-quatorze.', 'type': 'causal'}, {'question': 'What if Portugal had declared independence earlier than 1994? ', 'answer': \"It is possible that the country's political and social landscape would have been different.\", 'type': 'hypothetical'}, {'question': 'What country is the document from? ', 'answer': 'The document is from the Portuguese Republic.', 'type': 'complex'}]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_documents[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elqm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
