{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dynaconf import Dynaconf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from elqm import ELQMPipeline\n",
    "from elqm.eval import Evaluation\n",
    "from elqm.utils import get_dir\n",
    "from elqm.eval.oracle import generate_oracle_dataset\n",
    "\n",
    "from transformers import logging as trf_logging\n",
    "trf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2024_03_03\n",
    "\n",
    "# Set to None to generate a new oracle dataset for each index and save it in the cache\n",
    "DATASET_PATH = [\n",
    "    os.path.join(get_dir(\"data\", \"elqm-raw\", \"oracle\"), \"manual_qa_pairs.csv\"),\n",
    "    os.path.join(get_dir(\"data\", \"elqm-raw\", \"oracle\"), \"gpt4_collection.csv\"),\n",
    "    None\n",
    "][0]\n",
    "\n",
    "N_DOCUMENTS = 100  # How many question-answer pairs to generate if DATASET is None\n",
    "\n",
    "# This creates the name of the directory that the results will be save in: /results/<results_name>/<config_name>.csv\n",
    "results_name = os.path.splitext(os.path.basename(DATASET_PATH))[0] if DATASET_PATH is not None else f\"self_{N_DOCUMENTS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/psaegert/Projects/elqm-INLPT-WS2023/data/elqm-raw/oracle/manual_qa_pairs.csv\n",
      "manual_qa_pairs\n"
     ]
    }
   ],
   "source": [
    "print(DATASET_PATH)\n",
    "print(results_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 1024_10_nlc_bge.yaml\n",
      "âœ… 1024_1_nlc_bge.yaml\n",
      "âœ… 1024_5_nlc_bge.yaml\n",
      "âœ… 256_10_10_nlc_bge_fn_mistral.yaml\n",
      "âœ… 256_10_bge.yaml\n",
      "âœ… 256_10_g4a.yaml\n",
      "âœ… 256_10_nlc_bge.yaml\n",
      "âœ… 256_10_nlc_bge_fn.yaml\n",
      "âœ… 256_10_nlc_bge_fn_enrich.yaml\n",
      "âœ… 256_10_nlc_bge_fn_gemma.yaml\n",
      "âœ… 256_10_nlc_bge_fn_mistral.yaml\n",
      "âœ… 256_10_nlc_bge_fn_mistral_enrich.yaml\n",
      "âœ… 256_10_nlc_bge_fn_mixtral.yaml\n",
      "âœ… 256_10_nlc_bge_fn_orca2.yaml\n",
      "âœ… 256_10_nlc_bge_mistral.yaml\n",
      "âœ… 256_10_nlc_g4a.yaml\n",
      "âœ… 256_10_nlc_g4a_fn.yaml\n",
      "âœ… 256_1_nlc_bge.yaml\n",
      "âœ… 256_20_g4a_nomodel.yaml\n",
      "âœ… 256_5_5_nlc_bge_fn.yaml\n",
      "âœ… 256_5_5_nlc_bge_fn_mistral_h1.yaml\n",
      "âœ… 256_5_5_nlc_bge_fn_mistral_h2.yaml\n",
      "âœ… 256_5_5_nlc_bge_fn_mistral_hc1.yaml\n",
      "âœ… 256_5_5_nlc_bge_fn_mistral_hc2.yaml\n",
      "âœ… 256_5_5_nlc_bge_fn_mistral_hc3.yaml\n",
      "âœ… 256_5_5_nlc_bge_fn_mistral_s1.yaml\n",
      "âœ… 256_5_5_nlc_bge_fn_mistral_s2.yaml\n",
      "âœ… 256_5_5_nlc_bge_mistral.yaml\n",
      "âœ… 256_5_nlc_bge.yaml\n",
      "âœ… 256_bm_10_nlc_bge_fn_mistral.yaml\n",
      "âœ… 512_10_nlc_bge.yaml\n",
      "âœ… 512_1_nlc_bge.yaml\n",
      "âœ… 512_5_nlc_bge.yaml\n",
      "âœ… sem_40_nlc_bge_fn.yaml\n",
      "âœ… sem_40_nlc_bge_fn_mistral.yaml\n"
     ]
    }
   ],
   "source": [
    "config_results = {}\n",
    "config_paths = {}\n",
    "\n",
    "for root, dirs, files in os.walk(get_dir(\"configs\")):\n",
    "    for file in files:\n",
    "        if file.endswith(\".yaml\"):\n",
    "            config_path = os.path.join(root, file)\n",
    "            config_paths[file] = config_path\n",
    "\n",
    "            config = Dynaconf(settings_files=config_path)\n",
    "            if config.evaluate:\n",
    "                results_file = os.path.join(get_dir(\"results\", results_name, create=True), file.replace(\".yaml\", \".csv\"))\n",
    "                if os.path.exists(results_file):\n",
    "                    config_results[file] = pd.read_csv(results_file)\n",
    "                else:\n",
    "                    config_results[file] = None\n",
    "\n",
    "# Sort the dicts alphabetically\n",
    "config_paths = dict(sorted(config_paths.items()))\n",
    "config_results = dict(sorted(config_results.items()))\n",
    "\n",
    "for k, v in config_results.items():\n",
    "    print(\"âœ…\" if v is not None else \"ðŸ”²\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results = {k: v.mean() for k, v in config_results.items() if v is not None}\n",
    "\n",
    "if len(mean_results) > 0:\n",
    "    mean_results_df = pd.DataFrame(mean_results)\n",
    "    mean_results_df.loc[['A_BERT_RAG_F1', 'A_BERT_F1', 'A_AR_F1', 'RET_AP_5', 'RET_RR', 'RET_NDCG']].T.sort_values('A_BERT_RAG_F1', ascending=False).round(3)\n",
    "else:\n",
    "    print(\"No results yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_evaluation_data(elqm: ELQMPipeline, dataset_path: str | None) -> pd.DataFrame:\n",
    "    if dataset_path is None:\n",
    "        print(\"No dataset specified. Using auto-generated oracle dataset.\")\n",
    "        # Generate the oracle dataset if it doesn't exist\n",
    "        if not os.path.exists(os.path.join(get_dir(\"cache\", elqm.config.index_name), \"oracle_dataset.csv\")):\n",
    "            print(f\"Loading documents\")\n",
    "            documents = elqm.loader.load()\n",
    "\n",
    "            print(f\"Sampling {N_DOCUMENTS} documents\")\n",
    "            np.random.seed(RANDOM_SEED)\n",
    "            sampled_documents = np.random.choice(documents, N_DOCUMENTS, replace=False)\n",
    "\n",
    "            oracle_df = generate_oracle_dataset(sampled_documents, n_questions_per_type=1, strategy=\"random\", random_seed=RANDOM_SEED, verbose=True)\n",
    "\n",
    "            oracle_document_df_rows = []\n",
    "            for document in oracle_df:\n",
    "                for oracle_pair in document.metadata[\"oracle_pairs\"]:\n",
    "                    oracle_document_df_rows.append({\n",
    "                        \"source\": document.metadata[\"ID\"],\n",
    "                        \"type\": oracle_pair[\"type\"],\n",
    "                        \"question\": oracle_pair[\"question\"],\n",
    "                        \"answer\": oracle_pair[\"answer\"]})\n",
    "\n",
    "            oracle_df = pd.DataFrame(oracle_document_df_rows)\n",
    "\n",
    "            print(f\"Saving oracle dataset {elqm.config.index_name}\")\n",
    "            oracle_df.to_csv(os.path.join(get_dir(\"cache\", elqm.config.index_name), \"oracle_dataset.csv\"), index=False)\n",
    "        else:\n",
    "            print(f\"Loading oracle dataset {elqm.config.index_name}\")\n",
    "            oracle_df = pd.read_csv(os.path.join(get_dir(\"cache\", elqm.config.index_name), \"oracle_dataset.csv\"))\n",
    "    else:\n",
    "        print(f\"Using dataset {dataset_path}\")\n",
    "        oracle_df = pd.read_csv(dataset_path)\n",
    "\n",
    "    return oracle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already evaluated 1024_10_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 1024_1_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 1024_5_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_10_nlc_bge_fn_mistral. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_g4a. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge_fn. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge_fn_enrich. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge_fn_gemma. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge_fn_mistral. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge_fn_mistral_enrich. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge_fn_mixtral. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge_fn_orca2. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_bge_mistral. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_g4a. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_10_nlc_g4a_fn. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_1_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_20_g4a_nomodel. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_fn. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_fn_mistral_h1. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_fn_mistral_h2. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_fn_mistral_hc1. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_fn_mistral_hc2. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_fn_mistral_hc3. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_fn_mistral_s1. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_fn_mistral_s2. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_5_nlc_bge_mistral. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_5_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 256_bm_10_nlc_bge_fn_mistral. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 512_10_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 512_1_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated 512_5_nlc_bge. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated sem_40_nlc_bge_fn. Skipping.\n",
      "--------------------------------------------------------------------------------\n",
      "Already evaluated sem_40_nlc_bge_fn_mistral. Skipping.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for config_file in config_results.keys():\n",
    "    config_path = config_paths[config_file]\n",
    "    config_name = config_file.replace(\".yaml\", \"\")\n",
    "\n",
    "    # If the results already exist, skip\n",
    "    if config_results[config_file] is not None:\n",
    "        print(f\"Already evaluated {config_name}. Skipping.\")\n",
    "        print('-'*80)\n",
    "        continue\n",
    "\n",
    "    print(f\"Running {config_name}\")\n",
    "    elqm = ELQMPipeline(config=Dynaconf(settings_files=config_path))\n",
    "    \n",
    "    oracle_df = get_evaluation_data(elqm=elqm, dataset_path=DATASET_PATH)\n",
    "\n",
    "    # Evaluate the pipeline\n",
    "    results = eval.evaluate(elqm, oracle_df)\n",
    "\n",
    "    # Flatten the dicts\n",
    "    results['A_BL_BL'] = [r['bleu'] for r in results['A_BL']]\n",
    "    results['A_BL_PR'] = [np.mean(r['precisions']) for r in results['A_BL']]\n",
    "    results['A_BL_BREV'] = [r['brevity_penalty'] for r in results['A_BL']]\n",
    "    results['A_BL_LR'] = [r['length_ratio'] for r in results['A_BL']]\n",
    "    results['A_BL_LEN'] = [r['translation_length'] for r in results['A_BL']]\n",
    "    results['A_BL_REF_LEN'] = [r['reference_length'] for r in results['A_BL']]\n",
    "    del results['A_BL']\n",
    "\n",
    "    results['A_RG1'] = [r['rouge1'] for r in results['A_RG']]\n",
    "    results['A_RG2'] = [r['rouge2'] for r in results['A_RG']]\n",
    "    results['A_RGL'] = [r['rougeL'] for r in results['A_RG']]\n",
    "    results['A_RGL_SUM'] = [r['rougeLsum'] for r in results['A_RG']]\n",
    "    del results['A_RG']\n",
    "\n",
    "    results['A_BERT_RAG_PR'] = [r['precision'][0] for r in results['A_BERT_RAG']]\n",
    "    results['A_BERT_RAG_RC'] = [r['recall'][0] for r in results['A_BERT_RAG']]\n",
    "    results['A_BERT_RAG_F1'] = [r['f1'][0] for r in results['A_BERT_RAG']]\n",
    "    del results['A_BERT_RAG']\n",
    "\n",
    "    results['A_BERT_PR'] = [r['precision'][0] for r in results['A_BERT']]\n",
    "    results['A_BERT_RC'] = [r['recall'][0] for r in results['A_BERT']]\n",
    "    results['A_BERT_F1'] = [r['f1'][0] for r in results['A_BERT']]\n",
    "    del results['A_BERT']\n",
    "\n",
    "    results['A_AR_PR'] = [r[1]['precision'] for r in results['A_AR']]\n",
    "    results['A_AR_RC'] = [r[1]['recall'] for r in results['A_AR']]\n",
    "    results['A_AR_F1'] = [r[1]['f1'] for r in results['A_AR']]\n",
    "    del results['A_AR']\n",
    "\n",
    "    # Split the k values into separate columns\n",
    "    for split_column in ['RET_RC', 'RET_PR', 'RET_F1', 'RET_AP']:\n",
    "        # For each k value\n",
    "        for k in range(len(results[split_column][0])):\n",
    "            # New column is named after k and includes the value at that k for all examples\n",
    "            results[f'{split_column}_{k + 1}'] = [results[split_column][i][k] for i in range(len(results[split_column]))]\n",
    "        del results[split_column]\n",
    "\n",
    "    config_results[config_file] = pd.DataFrame(results)\n",
    "    config_results[config_file].to_csv(os.path.join(get_dir(\"results\", results_name), f\"{config_name}.csv\"), index=False)\n",
    "\n",
    "    mean_results = {k: v.mean() for k, v in config_results.items() if v is not None}\n",
    "\n",
    "    mean_results_df = pd.DataFrame(mean_results)\n",
    "\n",
    "    print(mean_results_df.loc[['A_BERT_RAG_F1', 'A_BERT_F1', 'A_AR_F1', 'RET_AP_5', 'RET_RR', 'RET_NDCG']].T.sort_values('A_BERT_RAG_F1', ascending=False).round(3))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elqm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
